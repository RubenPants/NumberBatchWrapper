{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "animated-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "large-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 355 µs (started: 2021-03-09 17:51:17 +01:00)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closing-rental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rubenbroekx/Documents/Projects/NumberBatchWrapper\n",
      "time: 1.21 ms (started: 2021-03-09 17:51:17 +01:00)\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-cornwall",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "This notebook shows the basic usage of the *Number Batch Wrapper* model, used to encode textual data into multilingual word or sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-german",
   "metadata": {},
   "source": [
    "## Load Wrapper and data\n",
    "\n",
    "Load in the *Number Batch Wrapper* model together with the dummy data used in this demonstration. The data is a list of 1000 English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "marked-ancient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: \n",
      "'He added that people should not mess with mother nature , and let sharks be .'\n",
      "time: 2.13 ms (started: 2021-03-09 17:51:17 +01:00)\n"
     ]
    }
   ],
   "source": [
    "# Load the demo-data\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path.cwd() / 'demos/data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(f\"Example sentence: \\n'{data[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-cache",
   "metadata": {},
   "source": [
    "It is possible to create your own text cleaner and tokenizer functions:\n",
    " - **text cleaning** is used when looking up the words in the NumberBatch dictionary\n",
    " - **tokenizing** is performed to split a given sentence up in words, which are cleaned and looked up in the NumberBatch dictionary\n",
    " \n",
    "Note that text cleaning is performed during model initialisation as well. This implies that if a different text cleaning function is provided to the model without re-running the `initialise` method again, some hazards might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organizational-hollywood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 953 ms (started: 2021-03-09 17:51:17 +01:00)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install fold-to-ascii stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "identical-velvet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and tokenizing: 'This is an example sentence!'\n",
      " ==> '['example', 'sentence']'\n",
      "time: 7.23 ms (started: 2021-03-09 17:51:18 +01:00)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from fold_to_ascii import fold\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "def clean(x:str) -> str:\n",
    "    \"\"\"Custom cleaning function.\"\"\"\n",
    "    x = fold(x.lower())\n",
    "    x = re.sub(r'[^a-z]', '', x)\n",
    "    return x\n",
    "\n",
    "STOP_EN = set(get_stop_words('en'))\n",
    "\n",
    "def tokenize(sentence:str) -> List[str]:\n",
    "    \"\"\"Custom tokenizer.\"\"\"\n",
    "    sentence = re.split(r'\\W', sentence)\n",
    "    sentence = [clean(w) for w in sentence]\n",
    "    sentence = [w for w in sentence if len(w) > 1 and w not in STOP_EN]\n",
    "    return sentence\n",
    "\n",
    "sentence = \"This is an example sentence!\"\n",
    "print(f\"Cleaning and tokenizing: '{sentence}'\")\n",
    "print(f\" ==> '{tokenize(sentence)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-philip",
   "metadata": {},
   "source": [
    "Set up the Number Batch Wrapper model using our custom cleaning and tokenization functions. Other default values (not shown) include:\n",
    " - `en_fallback` whether or not to fallback to English if word not found. Not applicable for this use-case since the default language is English.\n",
    " - `normalise` whether or not to normalise the resulting sentence embeddings.\n",
    " - `level` segmentation depth of the files, which is explained in more detail in the `2_performance.ipynb` notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "parental-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 157 ms (started: 2021-03-09 17:51:18 +01:00)\n"
     ]
    }
   ],
   "source": [
    "from number_batch_wrapper import Wrapper\n",
    "\n",
    "wrapper = Wrapper(\n",
    "    language='en',\n",
    "    path=Path.home() / 'numberbatch',\n",
    "    clean_f=clean,\n",
    "    tokenizer=tokenize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-hunger",
   "metadata": {},
   "source": [
    "Initialse the model, if this is not done before. For every configuration, `initialise` should be run only once your machine, since the results are cached under the folder specified by `wrapper.path`. The `inp_path` parameters specifies the folder where the Number Batch data is stored, or where to download the Number Batch data to. Note that this file is rather big (~3GB), so the download might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "facial-crash",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting 'en'..: 9161913it [01:12, 126545.97it/s]                             \n",
      "Segmenting 'en'..: 0it [00:57, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 11s (started: 2021-03-09 17:51:18 +01:00)\n"
     ]
    }
   ],
   "source": [
    "if not wrapper.is_initialised():\n",
    "    wrapper.initialise(\n",
    "        inp_path=Path.home() / 'Downloads'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-highland",
   "metadata": {},
   "source": [
    "## Encode and analyse\n",
    "\n",
    "Use the wrapper to encode the dummy sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exact-colors",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 1000/1000 [00:07<00:00, 140.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.37 s (started: 2021-03-09 17:53:30 +01:00)\n"
     ]
    }
   ],
   "source": [
    "results = wrapper(data)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-creator",
   "metadata": {},
   "source": [
    "For demonstration purposes, we search for the two most similar sentences as specified by their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adaptive-wagner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.1 s (started: 2021-03-09 17:53:37 +01:00)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "italic-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 556 ms (started: 2021-03-09 17:53:38 +01:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(results)\n",
    "np.fill_diagonal(similarity, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lightweight-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two most similar sentences (cosine similarity of 0.77776)\n",
      " - General and administrative expenses on a consolidated basis increased 24 % to approximately $ 2.7 million ( vs. approximately $ 2.2 million ) due to higher employee costs , an increase in the for doubtful retail accounts primarily from one hotel that was damaged by Hurricane Paloma last year , and higher professional fees .\n",
      " - Income tax expense increased $ 32.0 million during the first quarter of 2010 compared to 2009 . The effective tax rate increased to 35.2 percent in the first quarter of 2010 compared to 33.3 percent in 2009 , reflecting the higher effective rate associated with the Folgers business and the net favorable resolution of previously open tax positions in 2009 as compared to 2010 .\n",
      "time: 1.66 ms (started: 2021-03-09 17:53:39 +01:00)\n"
     ]
    }
   ],
   "source": [
    "best_sim = similarity.argmax()\n",
    "idx1 = best_sim // len(results)\n",
    "idx2 = best_sim % len(results)\n",
    "\n",
    "print(f\"Two most similar sentences (cosine similarity of {round(similarity[idx1,idx2],5):.5f})\")\n",
    "print(f\" - {data[idx1]}\")\n",
    "print(f\" - {data[idx2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-lindsay",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
